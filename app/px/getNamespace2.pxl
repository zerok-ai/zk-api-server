# Copyright 2018- The Pixie Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# SPDX-License-Identifier: Apache-2.0

import px
import pxviews


def get_namespace_data(start_time: str):
    df = px.DataFrame(table='process_stats', start_time=start_time)
    df.namespace = df.ctx['namespace']
    df = df.groupby('namespace').agg()
    df = df[['namespace']]

    return df

def get_pod_data(start_time: str):
    df = px.DataFrame(table='process_stats', start_time=start_time)
    df.namespace = df.ctx['namespace']
    df.pod = df.ctx['pod']

    df = df.groupby(['namespace', 'pod']).agg()
    df = df[df.namespace == 'default']
    df = df[['pod']]

    return df

def get_service_data(start_time: str):
    df = px.DataFrame(table='process_stats', start_time=start_time)
    df.namespace = df.ctx['namespace']
    df.service = df.ctx['service']

    df = df.groupby(['namespace', 'service']).agg()
    df = df[df.namespace == 'default']
    df = df[['service']]

    return df

def get_workload_data(start_time: str):
    df = px.DataFrame(table='process_stats', start_time=start_time)
    df.namespace = df.ctx['namespace']
    df.workload = df.ctx['workload']

    df = df.groupby(['namespace', 'workload']).agg()
    df = df[df.namespace == 'default']
    df = df[['workload']]

    return df

def get_resource_data(start_time: str, resource: str):
    df = px.DataFrame(table='process_stats', start_time=start_time)
    df.namespace = df.ctx['namespace']
    df.resource = df.ctx[resource]

    df = df.groupby(['namespace', resource]).agg()
    df = df[df.namespace == 'default']
    df = df[[resource]]

    return df

def service_let_summary(start_time: str):
    ''' Compute a summary of traffic by requesting service, for requests
        on services in the current cluster.
    Args:
    @start_time: The timestamp of data to start at.
    '''
    conn_stats_df = pxviews.connection_throughput_stats(start_time, px.now()).drop('time_')
    conn_stats_df.service = conn_stats_df.ctx['service']
    conn_stats_df = conn_stats_df.groupby(['service']).agg(
        inbound_conn_throughput=('inbound_conn_throughput', px.sum),
        outbound_conn_throughput=('outbound_conn_throughput', px.sum),
    )

    window = px.DurationNanos(px.now() - (px.now() + px.parse_duration(start_time)))
    conn_stats_df.inbound_conns = conn_stats_df.inbound_conn_throughput / window
    conn_stats_df.outbound_conns = conn_stats_df.outbound_conn_throughput / window

    http_stats_df = pxviews.inbound_http_summary(start_time=start_time, end_time=px.now())
    http_stats_df.service = http_stats_df.ctx['service']

    http_stats_df = http_stats_df.groupby(['service']).agg(
        http_req_count_in=('num_requests', px.sum),
        http_error_count_in=('num_errors', px.sum),
        # TODO usse a combine_quantiles UDF to merge quantiles
        http_latency_in=('latency_quantiles', px.any),
    )

    # Compute throughput values.
    http_stats_df.http_req_throughput_in = http_stats_df.http_req_count_in / window
    http_stats_df.http_error_rate_in = px.Percent(
        px.select(
            http_stats_df.http_req_count_in != 0,
            http_stats_df.http_error_count_in / http_stats_df.http_req_count_in,
            0.0,
        )
    )

    # Merge conn_stats_df and http_stats_df.
    df = conn_stats_df.merge(http_stats_df,
                             how='left',
                             left_on='service',
                             right_on='service',
                             suffixes=['', '_x'])

    return df[['service', 'http_latency_in', 'http_req_throughput_in', 'http_error_rate_in',
               'inbound_conns', 'outbound_conns']]

def my_fun():
    pod_count = pxviews.container_process_summary(px.now() + px.parse_duration("-5s"), px.now())
    pod_count = pod_count.groupby(['service', 'pod', 'namespace']).agg()
    pod_count = pod_count[pod_count.service != '']
    pod_count = pod_count.groupby('service').agg(pod_count=('pod', px.count))
    service_let = service_let_summary('-5s')
    df = pod_count.merge(
        service_let,
        how="left",
        left_on="service",
        right_on="service",
        suffixes=["", "_x"],
    )
    return df[['service', 'pod_count', 'http_latency_in', 'http_req_throughput_in', 'http_error_rate_in',
                   'inbound_conns', 'outbound_conns']]

def service_let_graph(start_time: str):
    ''' Compute a summary of traffic by requesting service, for requests on services
        in the current cluster.
    Args:
    @start_time: The timestamp of data to start at.
    '''
    df = pxviews.http_graph(start_time, px.now())
    df.window = px.DurationNanos(px.now() - px.parse_time(start_time))
    # Compute statistics about each edge of the service graph.
    df.request_throughput = df.num_requests / df.window
    df.inbound_throughput = df.req_bytes / df.window
    df.outbound_throughput = df.resp_bytes / df.window
    df.throughput_total = df.num_requests
    df.error_rate = px.Percent(df.num_errors / df.num_requests)

    return df[[
        'responder_pod',
        'requestor_pod',
        'responder_service',
        'requestor_service',
        'responder_ip',
        'requestor_ip',
        'latency_p50',
        'latency_p90',
        'latency_p99',
        'request_throughput',
        'error_rate',
        'inbound_throughput',
        'outbound_throughput',
        'throughput_total'
    ]]

px.display(get_namespace_data("{{.StartTime}}"), 'Namespace data')
