import px
import pxviews

ns_per_ms = 1000 * 1000
ns_per_s = 1000 * ns_per_ms
# Window size to use on time_ column for bucketing.
window_ns = px.DurationNanos(10 * ns_per_s)
# Flag to filter out requests that come from an unresolvable IP.
filter_unresolved_inbound = True
# Flag to filter out health checks from the data.
filter_health_checks = True
# Flag to filter out ready checks from the data.
filter_ready_checks = True
# Whether or not to include traffic from IPs that don't resolve to a known pod/service.
include_ips = True

def get_namespace_data(start_time: str):
    df = px.DataFrame(table='process_stats', start_time=start_time)
    df.namespace = df.ctx['namespace']
    df = df.groupby('namespace').agg()
    df = df[['namespace']]

    return df

def my_fun(start_time: str):
    pod_count = pxviews.container_process_summary(px.now() + px.parse_duration(start_time), px.now())
    pod_count = pod_count.groupby(['service', 'pod', 'namespace']).agg()
    pod_count = pod_count[pod_count.service != '']
    pod_count = pod_count.groupby('service').agg(pod_count=('pod', px.count))
    service_let = service_let_summary(start_time)
    df = pod_count.merge(
        service_let,
        how="left",
        left_on="service",
        right_on="service",
        suffixes=["", "_x"],
    )
    return df[['service', 'pod_count', 'http_latency_in', 'http_req_throughput_in', 'http_error_rate_in',
                   'inbound_conns', 'outbound_conns']]

def nodes_for_cluster(start_time: str):
    ''' Gets a list of nodes in the current cluster since `start_time`.
    Args:
    @start_time Start time of the data to examine.
    '''
    df = pxviews.container_process_summary(px.now() + px.parse_duration(start_time), px.now())
    agg = df.groupby(['node', 'pod']).agg()
    pod_per_node_count = agg.groupby('node').agg(pod_count=('pod', px.count))
    df = df.groupby(['node']).agg(
        cpu_usage=('cpu_usage', px.sum),
    )
    df.cpu_usage = px.Percent(df.cpu_usage)
    output = df.merge(pod_per_node_count, how='right', left_on='node', right_on='node',
                      suffixes=['', '_x'])
    return output[['node', 'cpu_usage', 'pod_count']]


def pods_for_cluster(start_time: str):
    ''' A list of pods in the cluster.
    Args:
    @start_time: The timestamp of data to start at.
    '''
    df = pxviews.pod_resource_stats(px.now() + px.parse_duration(start_time), px.now())
    df.start_time = df.pod_start_time
    df.status = df.pod_status
    return df[[
        'pod', 'cpu_usage', 'total_disk_read_throughput',
        'total_disk_write_throughput', 'container_count',
        'node', 'start_time', 'status',
    ]]


def namespaces_for_cluster(start_time: str):
    ''' Gets a overview of namespaces in the current cluster since `start_time`.
    Args:
    @start_time Start time of the data to examine.
    '''
    df = pxviews.container_process_summary(px.now() + px.parse_duration(start_time), px.now())
    agg = df.groupby(['service', 'pod', 'namespace']).agg()
    pod_count = agg.groupby(['namespace', 'pod']).agg()
    pod_count = pod_count.groupby('namespace').agg(pod_count=('pod', px.count))
    svc_count = agg.groupby(['namespace', 'service']).agg()
    svc_count = svc_count.groupby('namespace').agg(service_count=('service', px.count))
    pod_and_svc_count = pod_count.merge(svc_count, how='inner',
                                        left_on='namespace', right_on='namespace',
                                        suffixes=['', '_x'])
    df = df.groupby(['namespace']).agg(
        vsize=('vsize', px.sum),
        rss=('rss', px.sum),
    )
    output = df.merge(pod_and_svc_count, how='inner', left_on='namespace',
                      right_on='namespace', suffixes=['', '_y'])
    return output[['namespace', 'pod_count', 'service_count', 'vsize', 'rss']]


def services_for_cluster(start_time: str):
    ''' Get an overview of the services in the current cluster.
    Args:
    @start_time: The timestamp of data to start at.
    '''
    pod_count = pxviews.container_process_summary(px.now() + px.parse_duration(start_time), px.now())
    pod_count = pod_count.groupby(['service', 'pod', 'namespace']).agg()
    pod_count = pod_count[pod_count.service != '']
    pod_count = pod_count.groupby('service').agg(pod_count=('pod', px.count))

    service_let = service_let_summary(start_time)
    df = pod_count.merge(
        service_let,
        how="left",
        left_on="service",
        right_on="service",
        suffixes=["", "_x"],
    )
    return df[['service', 'pod_count', 'http_latency_in', 'http_req_throughput_in', 'http_error_rate_in',
               'inbound_conns', 'outbound_conns']]


def mysql_flow_graph(start_time: str):

    df = px.DataFrame('mysql_events', start_time=start_time)
    df = add_source_dest_columns(df)

    # Create 10 ns bin for time_ column
    df.timestamp = px.bin(df.time_, window_ns)

    df = df.groupby(['timestamp', 'source', 'destination', 'is_source_pod_type',
                     'is_dest_pod_type', 'namespace']).agg(
        latency_quantiles=('latency', px.quantiles),
        throughput_total=('latency', px.count),
    )

    df.latency_p50 = px.DurationNanos(px.floor(px.pluck_float64(df.latency_quantiles, 'p50')))
    df.latency_p90 = px.DurationNanos(px.floor(px.pluck_float64(df.latency_quantiles, 'p90')))
    df.latency_p99 = px.DurationNanos(px.floor(px.pluck_float64(df.latency_quantiles, 'p99')))
    df.request_throughput = df.throughput_total / window_ns

    df = df.groupby(['source', 'destination', 'is_source_pod_type', 'is_dest_pod_type',
                     'namespace']).agg(
        latency_p50=('latency_p50', px.mean),
        latency_p90=('latency_p90', px.mean),
        latency_p99=('latency_p99', px.mean),
        request_throughput=('request_throughput', px.mean),
        throughput_total=('throughput_total', px.sum)
    )

    return df


def service_let_summary(start_time: str):
    ''' Compute a summary of traffic by requesting service, for requests
        on services in the current cluster.
    Args:
    @start_time: The timestamp of data to start at.
    '''
    conn_stats_df = pxviews.connection_throughput_stats(start_time, px.now()).drop('time_')
    conn_stats_df.service = conn_stats_df.ctx['service']
    conn_stats_df = conn_stats_df.groupby(['service']).agg(
        inbound_conn_throughput=('inbound_conn_throughput', px.sum),
        outbound_conn_throughput=('outbound_conn_throughput', px.sum),
    )

    window = px.DurationNanos(px.now() - (px.now() + px.parse_duration(start_time)))
    conn_stats_df.inbound_conns = conn_stats_df.inbound_conn_throughput / window
    conn_stats_df.outbound_conns = conn_stats_df.outbound_conn_throughput / window

    http_stats_df = pxviews.inbound_http_summary(start_time=start_time, end_time=px.now())
    http_stats_df.service = http_stats_df.ctx['service']

    http_stats_df = http_stats_df.groupby(['service']).agg(
        http_req_count_in=('num_requests', px.sum),
        http_error_count_in=('num_errors', px.sum),
        # TODO usse a combine_quantiles UDF to merge quantiles
        http_latency_in=('latency_quantiles', px.any),
    )

    # Compute throughput values.
    http_stats_df.http_req_throughput_in = http_stats_df.http_req_count_in / window
    http_stats_df.http_error_rate_in = px.Percent(
        px.select(
            http_stats_df.http_req_count_in != 0,
            http_stats_df.http_error_count_in / http_stats_df.http_req_count_in,
            0.0,
        )
    )

    # Merge conn_stats_df and http_stats_df.
    df = conn_stats_df.merge(http_stats_df,
                             how='left',
                             left_on='service',
                             right_on='service',
                             suffixes=['', '_x'])

    return df[['service', 'http_latency_in', 'http_req_throughput_in', 'http_error_rate_in',
               'inbound_conns', 'outbound_conns']]


def mysql_summary_with_links(start_time: str):

    df = mysql_flow_graph(start_time)
    # df = add_source_dest_links(df, start_time)
    df = df[['source', 'destination', 'latency_p50', 'latency_p90',
            'latency_p99', 'request_throughput', 'throughput_total']]

    return df


def service_http_graph(start_time: str):
    ''' Compute a summary of traffic by requesting service, for requests on services
        in the current cluster.
    Args:
    @start_time: The timestamp of data to start at.
    '''
    df = pxviews.http_graph(start_time, px.now())
    df.window = px.DurationNanos(px.now() - px.parse_time(start_time))
    # Compute statistics about each edge of the service graph.
    df.request_throughput = df.num_requests / df.window
    df.inbound_throughput = df.req_bytes / df.window
    df.outbound_throughput = df.resp_bytes / df.window
    df.throughput_total = df.num_requests
    df.error_rate = px.Percent(df.num_errors / df.num_requests)

    return df[[
        'responder_pod',
        'requestor_pod',
        'responder_service',
        'requestor_service',
        'responder_ip',
        'requestor_ip',
        'latency_p50',
        'latency_p90',
        'latency_p99',
        'request_throughput',
        'error_rate',
        'inbound_throughput',
        'outbound_throughput',
        'throughput_total'
    ]]

def service_mysql_graph(start_time: str):
    mdf = mysql_summary_with_links(start_time)

    mdf.requestor_service = px.pod_name_to_service_name(mdf.source)
    mdf.responder_service = px.pod_name_to_service_name(mdf.destination)
    mdf.requestor_pod = mdf.source
    mdf.responder_pod = mdf.destination
    mdf.requestor_ip = px.pod_name_to_pod_ip(mdf.source)
    mdf.responder_ip = px.pod_name_to_pod_ip(mdf.destination)
    mdf.error_rate = 0.0
    mdf.inbound_throughput = 0.0
    mdf.outbound_throughput = 0.0

    return mdf[[
        'responder_pod',
        'requestor_pod',
        'responder_service',
        'requestor_service',
        'responder_ip',
        'requestor_ip',
        'latency_p50',
        'latency_p90',
        'latency_p99',
        'request_throughput',
        'error_rate',
        'inbound_throughput',
        'outbound_throughput',
        'throughput_total'
    ]]


def service_let_graph(start_time: str):
    hdf = service_http_graph(start_time)
    mdf = service_mysql_graph(start_time)

    final_df = mdf.append(hdf)

    return mdf

def inbound_let_timeseries(start_time: str, service: px.Service):
    ''' Compute the let as a timeseries for requests received by `service`.

    Args:
    @start_time: The timestamp of data to start at.
    @service: The name of the service to filter on.

    '''
    df = let_helper(start_time)
    df = df[px.has_service_name(df.service, service)]

    df = df.groupby(['timestamp']).agg(
        latency_quantiles=('latency', px.quantiles),
        error_rate_per_window=('failure', px.mean),
        throughput_total=('latency', px.count),
        bytes_total=('resp_body_size', px.sum)
    )

    # Format the result of LET aggregates into proper scalar formats and
    # time series.
    df.latency_p50 = px.DurationNanos(px.floor(px.pluck_float64(df.latency_quantiles, 'p50')))
    df.latency_p90 = px.DurationNanos(px.floor(px.pluck_float64(df.latency_quantiles, 'p90')))
    df.latency_p99 = px.DurationNanos(px.floor(px.pluck_float64(df.latency_quantiles, 'p99')))
    df.request_throughput = df.throughput_total / window_ns * 1000000000
    df.errors_per_ns = df.error_rate_per_window * df.request_throughput / px.DurationNanos(1)
    df.error_rate = px.Percent(df.error_rate_per_window)
    df.bytes_per_ns = df.bytes_total / window_ns
    df.time_ = df.timestamp

    return df[['time_', 'latency_p50', 'latency_p90', 'latency_p99',
               'request_throughput', 'errors_per_ns', 'error_rate', 'bytes_per_ns']]

def let_helper(start_time: str):
    ''' Compute the initial part of the let for requests.
        Filtering to inbound/outbound traffic by service is done by the calling function.

    Args:
    @start_time: The timestamp of data to start at.

    '''
    df = px.DataFrame(table='http_events', start_time=start_time)
    # Filter only to inbound service traffic (server-side).
    # Don't include traffic initiated by this service to an external location.
    df = df[df.trace_role == 2]
    df.service = df.ctx['service']
    df.pod = df.ctx['pod']
    df.latency = df.latency

    df.timestamp = px.bin(df.time_, window_ns)

    df.failure = df.resp_status >= 400
    filter_out_conds = ((df.req_path != '/healthz' or not filter_health_checks) and (
        df.req_path != '/readyz' or not filter_ready_checks)) and (
        df['remote_addr'] != '-' or not filter_unresolved_inbound)

    df = df[filter_out_conds]
    return df

def inbound_service_let_helper(start_time: str, namespace: px.Namespace):
    ''' Compute the let as a timeseries for requests received or by services in `namespace`.

    Args:
    @start_time: The timestamp of data to start at.
    @namespace: The namespace to filter on.

    '''
    df = px.DataFrame(table='http_events', start_time=start_time)
    df.service = df.ctx['service']
    df.pod = df.ctx['pod_name']
    df = df[df.ctx['namespace'] == namespace]
    df = df[df.pod != '']
    df.latency = df.latency
    df.timestamp = px.bin(df.time_, window_ns)

    df.failure = df.resp_status >= 400
    filter_out_conds = ((df.req_path != '/healthz' or not filter_health_checks) and (
        df.req_path != '/readyz' or not filter_ready_checks)) and (
        df['remote_addr'] != '-' or not filter_unresolved_inbound)

    df = df[filter_out_conds]
    return df



def apply_column_filter(df, colName, filterVal):
    # df[colName] = filterVal
    # return df
    df = df[px.contains(df[colName], filterVal)]
    return df

def prepare_column_filter(df, colName, filter):
    filterVal = px.select(px.length(px.pluck(filter, colName)) > 0, px.pluck(filter,colName), "")
    df = apply_column_filter(df, colName, filterVal)
    return df

def apply_filter(df, filter):
    df = prepare_column_filter(df, "type", filter)
    df = prepare_column_filter(df, "trace_id", filter)
    df = prepare_column_filter(df, "span_id", filter)
    df = prepare_column_filter(df, "otel_flag", filter)
    df = prepare_column_filter(df, "source", filter)
    df = prepare_column_filter(df, "destination", filter)
    df = prepare_column_filter(df, "req_body", filter)
    df = prepare_column_filter(df, "resp_body", filter)
    df = prepare_column_filter(df, "req_path", filter)
    df = prepare_column_filter(df, "req_method", filter)
    df = prepare_column_filter(df, "req_headers", filter)
    return df

def add_mysql_trace_parent(df):
    # filter rows containing traceparent before extracting trace data.
    df = df[px.contains(df.req_body, "/* traceparent:")]
    df.traceparent = px.substring(df.req_body, 15, 55)
    df.tracestate = px.substring(df.req_body, 82, 22)
    df = df[px.length(df.traceparent) > 0]
    return df

def add_http_trace_parent(df):
    req_traceparent=px.pluck(df.req_headers, 'traceparent')
    resp_traceparent=px.pluck(df.resp_headers, 'traceparent')
    req_tracestate=px.pluck(df.req_headers, 'tracestate')
    resp_tracestate=px.pluck(df.resp_headers, 'tracestate')
    df.traceparent = px.select(px.length(req_traceparent) > 0, req_traceparent, resp_traceparent)
    df.tracestate = px.select(px.length(req_tracestate) > 0, req_tracestate, resp_tracestate)

    # df = df[px.length(df.traceparent) > 0]
    return df

def add_telemetry_headers(df):
    df.trace_id = px.substring(df.traceparent, 3, 32)
    df.span_id = px.substring(df.traceparent, 36, 16)
    df.otel_flag = px.substring(df.traceparent, 53, 2)

    df.drop(['traceparent'])
    return df
    # df.str = px.substring(df.service, 1, 5)

def prepare_base_dataframe(table, start_time):
    df = px.DataFrame(table=table, start_time=start_time)
    df.node = df.ctx['node']
    df.pid = px.upid_to_pid(df.upid)
    df = add_source_dest_columns(df)
    df = add_source_dest_links(df, start_time)
    return df

def get_mysql_data(start_time: str):
    df = prepare_base_dataframe('mysql_events', start_time)
    df = df[px.length(df.resp_body) > 0]

    df = add_mysql_trace_parent(df)
    df = add_telemetry_headers(df)

    df['req_path'] = ""
    df['req_method'] = ""
    df['req_headers'] = ""
    df["type"] = "MYSQL"

    df = df['time_', 'type', 'tracestate', 'trace_id', 'span_id', 'otel_flag',
            'source', 'destination', 'latency', 'req_body', 'resp_body',
            'req_path', 'req_method', 'req_headers']

    return df

def get_exception_data(start_time: str):
    df = prepare_base_dataframe('http_events', start_time)

    # Filter out onlu exception requests.
    df = df[px.contains(df.destination, "zerok-operator-system/zerok-operator-controller") and px.contains(df.req_path, '/exception')]
    df = add_http_trace_parent(df)
    df = add_telemetry_headers(df)

    df["type"] = "EXCEPTION"

    # Order columns.
    df = df['time_', 'type', 'tracestate', 'trace_id', 'span_id', 'otel_flag',
            'source', 'destination', 'latency', 'req_body', 'resp_body',
            'req_path', 'req_method', 'req_headers']

    return df

def get_http_data(start_time: str):
    df = prepare_base_dataframe('http_events', start_time)

    # Filter out onlu exception requests.
    # df = df[px.contains(df.destination, "default")]
    df = df[px.contains(px.pluck(df.source,"script"), "px/pod")]

    df = add_http_trace_parent(df)
    df = add_telemetry_headers(df)

    df["type"] = "HTTP"

    # Order columns.
    df = df['time_', 'type', 'tracestate', 'trace_id', 'span_id', 'otel_flag',
            'source', 'destination', 'latency', 'req_body', 'resp_body',
            'req_path', 'req_method', 'req_headers']

    return df

def get_roi_data(start_time: str, limit: int, filter: str):
    mysql_df = get_mysql_data(start_time)
    http_df = get_http_data(start_time)
    exception_df = get_exception_data(start_time)

    data1_df = http_df.append(exception_df)
    roi_df = data1_df.append(mysql_df)

    roi_df = apply_filter(roi_df, filter)

    roi_df.head(limit)

    return roi_df


def add_source_dest_columns(df):
    ''' Add source and destination columns for the MySQL request.

    MySQL requests are traced server-side (trace_role==2), unless the server is
    outside of the cluster in which case the request is traced client-side (trace_role==1).

    When trace_role==2, the MySQL request source is the remote_addr column
    and destination is the pod column. When trace_role==1, the MySQL request
    source is the pod column and the destination is the remote_addr column.

    Input DataFrame must contain trace_role, upid, remote_addr columns.
    '''
    df.pod = df.ctx['pod']
    df.namespace = df.ctx['namespace']

    # If remote_addr is a pod, get its name. If not, use IP address.
    df.ra_pod = px.pod_id_to_pod_name(px.ip_to_pod_id(df.remote_addr))
    df.is_ra_pod = df.ra_pod != ''
    df.ra_name = px.select(df.is_ra_pod, df.ra_pod, df.remote_addr)

    df.is_server_tracing = df.trace_role == 2
    df.is_source_pod_type = px.select(df.is_server_tracing, df.is_ra_pod, True)
    df.is_dest_pod_type = px.select(df.is_server_tracing, True, df.is_ra_pod)

    # Set source and destination based on trace_role.
    df.source = px.select(df.is_server_tracing, df.ra_name, df.pod)
    df.destination = px.select(df.is_server_tracing, df.pod, df.ra_name)

    # Filter out messages with empty source / destination.
    df = df[df.source != '']
    df = df[df.destination != '']

    df = df.drop(['ra_pod', 'is_ra_pod', 'ra_name', 'is_server_tracing'])

    return df


def add_source_dest_links(df, start_time: str):
    ''' Modifies the source and destination columns to display deeplinks in the UI.
    Clicking on a pod name in either column will run the px/pod script for that pod.
    Clicking on an IP address, will run the px/ip script showing all network connections
    to/from that IP address.

    Input DataFrame must contain source, destination, is_source_pod_type,
    is_dest_pod_type, and namespace columns.
    '''

    # Source linking. If source is a pod, link to px/pod. If an IP addr, link to px/net_flow_graph.
    df.src_pod_link = px.script_reference(df.source, 'px/pod', {
        'start_time': start_time,
        'pod': df.source
    })
    df.src_link = px.script_reference(df.source, 'px/ip', {
        'start_time': start_time,
        'ip': df.source,
    })
    df.source = px.select(df.is_source_pod_type, df.src_pod_link, df.src_link)

    # If destination is a pod, link to px/pod. If an IP addr, link to px/net_flow_graph.
    df.dest_pod_link = px.script_reference(df.destination, 'px/pod', {
        'start_time': start_time,
        'pod': df.destination
    })
    df.dest_link = px.script_reference(df.destination, 'px/ip', {
        'start_time': start_time,
        'ip': df.destination,
    })
    df.destination = px.select(df.is_dest_pod_type, df.dest_pod_link, df.dest_link)

    df = df.drop(['src_pod_link', 'src_link', 'is_source_pod_type', 'dest_pod_link',
                  'dest_link', 'is_dest_pod_type'])

    return df

def pods(start_time: str, namespace: px.Namespace, service: px.Service):

    df = px.DataFrame(table='process_stats', start_time=start_time)
    df = df[df.ctx['namespace'] == namespace]
    df = df[px.has_service_name(df.ctx['service'], service)]

    df.pod = df.ctx['pod_name']
    df.container = df.ctx['container_name']
    df.service = df.ctx['service']
    df.pod_name = df.ctx['pod']

    df = df.groupby(['service', 'pod', 'container']).agg()
    df = df.groupby(['service', 'pod']).agg(containers=('container', px.count))
    df.start_time = px.pod_name_to_start_time(df.pod)
    df.status = px.pod_name_to_status(df.pod)
    return df[['pod', 'service', 'start_time', 'containers', 'status']]

def pod_details_inbound_request_timeseries_by_container(start_time: str, pod: px.Pod):
    ''' Compute the request statistics as a timeseries for requests received
        by `pod` by container.

    Args:
    @start_time: The timestamp of data to start at.
    @pod: The name of the pod to filter on.

    '''
    df = filtered_http_events(start_time,
                              include_health_checks=not filter_health_checks,
                              include_ready_checks=not filter_ready_checks,
                              include_unresolved_inbound=not filter_unresolved_inbound)
    df.pod = df.ctx['pod']
    df = df[df.pod == pod]

    # Filter only to inbound pod traffic (server-side).
    # Don't include traffic initiated by this pod to an external location.
    df = df[df.trace_role == 2]

    df.container = df.ctx['container']
    df.timestamp = px.bin(df.time_, window_ns)
    df = df.groupby(['timestamp', 'container']).agg(
        error_rate_per_window=('failure', px.mean),
        throughput_total=('latency', px.count)
    )

    # Format the result of LET aggregates into proper scalar formats and
    # time series.
    df.request_throughput = df.throughput_total / window_ns
    df.errors_per_ns = df.error_rate_per_window * df.request_throughput / px.DurationNanos(1)
    df.error_rate = px.Percent(df.error_rate_per_window)
    df.time_ = df.timestamp
    return df[['time_', 'container', 'request_throughput', 'errors_per_ns', 'error_rate']]

def pod_details_inbound_latency_timeseries(start_time: str, pod: px.Pod):
    ''' Compute the latency as a timeseries for requests received by `pod`.

    Args:
    @start_time: The timestamp of data to start at.
    @pod: The name of the pod to filter on.

    '''
    df = filtered_http_events(start_time,
                              include_health_checks=not filter_health_checks,
                              include_ready_checks=not filter_ready_checks,
                              include_unresolved_inbound=not filter_unresolved_inbound)
    df.pod = df.ctx['pod']
    df = df[df.pod == pod]

    # Filter only to inbound pod traffic (server-side).
    # Don't include traffic initiated by this pod to an external location.
    df = df[df.trace_role == 2]

    df.timestamp = px.bin(df.time_, window_ns)
    df = df.groupby(['timestamp']).agg(
        latency_quantiles=('latency', px.quantiles)
    )

    # Format the result of LET aggregates into proper scalar formats and
    # time series.
    df.latency_p50 = px.DurationNanos(px.floor(px.pluck_float64(df.latency_quantiles, 'p50')))
    df.latency_p90 = px.DurationNanos(px.floor(px.pluck_float64(df.latency_quantiles, 'p90')))
    df.latency_p99 = px.DurationNanos(px.floor(px.pluck_float64(df.latency_quantiles, 'p99')))
    df.time_ = df.timestamp
    return df[['time_', 'latency_p50', 'latency_p90', 'latency_p99']]

def pod_details_resource_timeseries(start_time: str, pod: px.Pod):
    ''' Compute the resource usage as a timeseries for `pod`.

    Args:
    @start_time: The timestamp of data to start at.
    @pod: The name of the pod to filter on.

    '''
    df = px.DataFrame(table='process_stats', start_time=start_time)
    df = df[df.ctx['pod'] == pod]
    df.timestamp = px.bin(df.time_, window_ns)
    df.container = df.ctx['container_name']

    # First calculate CPU usage by process (UPID) in each k8s_object
    # over all windows.
    df = df.groupby(['upid', 'container', 'timestamp']).agg(
        rss=('rss_bytes', px.mean),
        vsize=('vsize_bytes', px.mean),
        # The fields below are counters, so we take the min and the max to subtract them.
        cpu_utime_ns_max=('cpu_utime_ns', px.max),
        cpu_utime_ns_min=('cpu_utime_ns', px.min),
        cpu_ktime_ns_max=('cpu_ktime_ns', px.max),
        cpu_ktime_ns_min=('cpu_ktime_ns', px.min),
        read_bytes_max=('read_bytes', px.max),
        read_bytes_min=('read_bytes', px.min),
        write_bytes_max=('write_bytes', px.max),
        write_bytes_min=('write_bytes', px.min),
        rchar_bytes_max=('rchar_bytes', px.max),
        rchar_bytes_min=('rchar_bytes', px.min),
        wchar_bytes_max=('wchar_bytes', px.max),
        wchar_bytes_min=('wchar_bytes', px.min),
    )


    # Next calculate cpu usage and memory stats per window.
    df.cpu_utime_ns = df.cpu_utime_ns_max - df.cpu_utime_ns_min
    df.cpu_ktime_ns = df.cpu_ktime_ns_max - df.cpu_ktime_ns_min
    df.actual_disk_read_throughput = (df.read_bytes_max - df.read_bytes_min) / window_ns
    df.actual_disk_write_throughput = (df.write_bytes_max - df.write_bytes_min) / window_ns
    df.total_disk_read_throughput = (df.rchar_bytes_max - df.rchar_bytes_min) / window_ns
    df.total_disk_write_throughput = (df.wchar_bytes_max - df.wchar_bytes_min) / window_ns

    # Then aggregate process individual process metrics.
    df = df.groupby(['timestamp', 'container']).agg(
        cpu_ktime_ns=('cpu_ktime_ns', px.sum),
        cpu_utime_ns=('cpu_utime_ns', px.sum),
        actual_disk_read_throughput=('actual_disk_read_throughput', px.sum),
        actual_disk_write_throughput=('actual_disk_write_throughput', px.sum),
        total_disk_read_throughput=('total_disk_read_throughput', px.sum),
        total_disk_write_throughput=('total_disk_write_throughput', px.sum),
        rss=('rss', px.sum),
        vsize=('vsize', px.sum),
    )

    # Finally, calculate total (kernel + user time)  percentage used over window.
    df.cpu_usage = px.Percent((df.cpu_ktime_ns + df.cpu_utime_ns) / window_ns)
    df.time_ = df.timestamp
    return df.drop(['cpu_ktime_ns', 'cpu_utime_ns', 'timestamp'])

def filtered_http_events(start_time: str, include_health_checks, include_ready_checks, include_unresolved_inbound):
    ''' Returns a dataframe of http events with health/ready requests filtered out and a
        failure field added.

    Args:
    @start_time: The timestamp of data to start at.

    '''
    df = px.DataFrame(table='http_events', start_time=start_time)
    df.failure = df.resp_status >= 400

    filter_out_conds = ((df.req_path != '/healthz' or include_health_checks) and (
        df.req_path != '/readyz' or include_ready_checks)) and (
        df['remote_addr'] != '-' or include_unresolved_inbound)
    df = df[filter_out_conds]

    return df

px.display({{.MethodSignature}}, '{{.DataFrameName}}')
